# HIPE 2022 Challenge Evaluation Results
TEST

<!--ts-->
<!--te-->


##Multilingual Newspaper Challenge (MNC)



###MNC: Overall ranking

|    | CHALLENGE   | RANK   | POINTS   | TEAM   |
|:---|:------------|:-------|:---------|:-------|
| 0  | MNC         | 1      | 460      | team4  |
| 1  | MNC         | 2      | 440      | team2  |
| 2  | MNC         | 3      | 330      | team5  |
| 3  | MNC         | 4      | 150      | team3  |

###MNC: Ranking overview for nel-only-relaxed 

|    | CHALLENGE   | RANK   | POINTS   | TEAM   |
|:---|:------------|:-------|:---------|:-------|
| 0  | MCC:EL-ONLY | 1      | 330      | team5  |
| 1  | MCC:EL-ONLY | 2      | 140      | team2  |

See [mnc-challenge-nel-only-relaxed-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mnc-challenge-nel-only-relaxed-challenge-team-ranking.tsv) for full details.

###MNC: Detailed view for nel-only-relaxed 

|    | CHALLENGE   | RANK   | POINTS   | TEAM   | DATASET    | LANGUAGE   | F1    | System                        |
|:---|:------------|:-------|:---------|:-------|:-----------|:-----------|:------|:------------------------------|
| 0  | MCC:EL-ONLY | 1      | 50       | team5  | hipe2020   | de         | 0.525 | team5_bundle5_hipe2020_de_1   |
| 1  | MCC:EL-ONLY | 2      | 40       | team2  | hipe2020   | de         | 0.497 | team2_bundle5_hipe2020_de_1   |
| 2  | MCC:EL-ONLY | 1      | 50       | team2  | hipe2020   | en         | 0.546 | team2_bundle5_hipe2020_en_1   |
| 3  | MCC:EL-ONLY | 2      | 40       | team5  | hipe2020   | en         | 0.393 | team5_bundle5_hipe2020_en_2   |
| 4  | MCC:EL-ONLY | 1      | 50       | team2  | hipe2020   | fr         | 0.620 | team2_bundle5_hipe2020_fr_1   |
| 5  | MCC:EL-ONLY | 2      | 40       | team5  | hipe2020   | fr         | 0.616 | team5_bundle5_hipe2020_fr_2   |
| 6  | MCC:EL-ONLY | 1      | 50       | team5  | newseye    | de         | 0.455 | team5_bundle5_newseye_de_1    |
| 7  | MCC:EL-ONLY | 1      | 50       | team5  | newseye    | fr         | 0.435 | team5_bundle5_newseye_fr_1    |
| 8  | MCC:EL-ONLY | 1      | 50       | team5  | sonar      | de         | 0.517 | team5_bundle5_sonar_de_1      |
| 9  | MCC:EL-ONLY | 1      | 50       | team5  | topres19th | en         | 0.654 | team5_bundle5_topres19th_en_2 |

See [mnc-challenge-nel-only-relaxed-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mnc-challenge-nel-only-relaxed-dataset-team-ranking.tsv) for full details.

###MNC: Ranking overview for nel-relaxed 

|    | CHALLENGE   | RANK   | POINTS   | TEAM   |
|:---|:------------|:-------|:---------|:-------|
| 0  | MNC:EL      | 1      | 150      | team2  |

See [mnc-challenge-nel-relaxed-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mnc-challenge-nel-relaxed-challenge-team-ranking.tsv) for full details.

###MNC: Detailed view for nel-relaxed 

|    | CHALLENGE   | RANK   | POINTS   | TEAM   | DATASET   | LANGUAGE   | F1    | System                      |
|:---|:------------|:-------|:---------|:-------|:----------|:-----------|:------|:----------------------------|
| 0  | MNC:EL      | 1      | 50       | team2  | hipe2020  | de         | 0.464 | team2_bundle1_hipe2020_de_2 |
| 1  | MNC:EL      | 1      | 50       | team2  | hipe2020  | en         | 0.469 | team2_bundle1_hipe2020_en_2 |
| 2  | MNC:EL      | 1      | 50       | team2  | hipe2020  | fr         | 0.578 | team2_bundle1_hipe2020_fr_1 |

See [mnc-challenge-nel-relaxed-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mnc-challenge-nel-relaxed-dataset-team-ranking.tsv) for full details.

###MNC: Ranking overview for nerc-coarse-fuzzy 

|    | CHALLENGE       | RANK   | POINTS   | TEAM   |
|:---|:----------------|:-------|:---------|:-------|
| 0  | MNC:NERC-COARSE | 1      | 460      | team4  |
| 1  | MNC:NERC-COARSE | 2      | 150      | team2  |
| 2  | MNC:NERC-COARSE | 3      | 150      | team3  |

See [mnc-challenge-nerc-coarse-fuzzy-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mnc-challenge-nerc-coarse-fuzzy-challenge-team-ranking.tsv) for full details.

###MNC: Detailed view for nerc-coarse-fuzzy 

|    | CHALLENGE       | RANK   | POINTS   | TEAM   | DATASET    | LANGUAGE   | F1    | System                        |
|:---|:----------------|:-------|:---------|:-------|:-----------|:-----------|:------|:------------------------------|
| 0  | MNC:NERC-COARSE | 1      | 50       | team2  | hipe2020   | de         | 0.876 | team2_bundle3_hipe2020_de_1   |
| 1  | MNC:NERC-COARSE | 2      | 40       | team4  | hipe2020   | de         | 0.822 | team4_bundle4_hipe2020_de_1   |
| 2  | MNC:NERC-COARSE | 1      | 50       | team2  | hipe2020   | en         | 0.788 | team2_bundle1_hipe2020_en_1   |
| 3  | MNC:NERC-COARSE | 2      | 40       | team4  | hipe2020   | en         | 0.692 | team4_bundle4_hipe2020_en_1   |
| 4  | MNC:NERC-COARSE | 3      | 30       | team3  | hipe2020   | en         | 0.603 | team3_bundle4_hipe2020_en_1   |
| 5  | MNC:NERC-COARSE | 1      | 50       | team2  | hipe2020   | fr         | 0.907 | team2_bundle1_hipe2020_fr_2   |
| 6  | MNC:NERC-COARSE | 2      | 40       | team3  | hipe2020   | fr         | 0.808 | team3_bundle4_hipe2020_fr_1   |
| 7  | MNC:NERC-COARSE | 3      | 30       | team4  | hipe2020   | fr         | 0.800 | team4_bundle4_hipe2020_fr_2   |
| 8  | MNC:NERC-COARSE | 1      | 50       | team4  | letemps    | fr         | 0.701 | team4_bundle4_letemps_fr_1    |
| 9  | MNC:NERC-COARSE | 2      | 40       | team3  | letemps    | fr         | 0.666 | team3_bundle4_letemps_fr_1    |
| 10 | MNC:NERC-COARSE | 1      | 50       | team4  | newseye    | de         | 0.495 | team4_bundle4_newseye_de_2    |
| 11 | MNC:NERC-COARSE | 1      | 50       | team4  | newseye    | fi         | 0.670 | team4_bundle4_newseye_fi_1    |
| 12 | MNC:NERC-COARSE | 1      | 50       | team4  | newseye    | fr         | 0.786 | team4_bundle4_newseye_fr_2    |
| 13 | MNC:NERC-COARSE | 1      | 50       | team4  | newseye    | sv         | 0.746 | team4_bundle4_newseye_sv_1    |
| 14 | MNC:NERC-COARSE | 1      | 50       | team4  | sonar      | de         | 0.695 | team4_bundle4_sonar_de_2      |
| 15 | MNC:NERC-COARSE | 1      | 50       | team4  | topres19th | en         | 0.838 | team4_bundle4_topres19th_en_1 |
| 16 | MNC:NERC-COARSE | 2      | 40       | team3  | topres19th | en         | 0.796 | team3_bundle4_topres19th_en_1 |

See [mnc-challenge-nerc-coarse-fuzzy-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mnc-challenge-nerc-coarse-fuzzy-dataset-team-ranking.tsv) for full details.

##Multilingual Classical Commentary Challenge (MCC)



###MCC: Overall ranking

|    | CHALLENGE   | RANK   | POINTS   | TEAM   |
|:---|:------------|:-------|:---------|:-------|
| 0  | MCC         | 1      | 180      | team2  |
| 1  | MCC         | 2      | 150      | team5  |
| 2  | MCC         | 3      | 140      | team1  |

###MCC: Ranking overview for nel-only-relaxed 

|    | CHALLENGE   | RANK   | POINTS   | TEAM   |
|:---|:------------|:-------|:---------|:-------|
| 0  | MCC:EL-ONLY | 1      | 150      | team5  |

See [mcc-challenge-nel-only-relaxed-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mcc-challenge-nel-only-relaxed-challenge-team-ranking.tsv) for full details.

###MCC: Detailed view for nel-only-relaxed 

|    | CHALLENGE   | RANK   | POINTS   | TEAM   | DATASET   | LANGUAGE   | F1    | System                  |
|:---|:------------|:-------|:---------|:-------|:----------|:-----------|:------|:------------------------|
| 0  | MCC:EL-ONLY | 1      | 50       | team5  | ajmc      | de         | 0.503 | team5_bundle5_ajmc_de_2 |
| 1  | MCC:EL-ONLY | 1      | 50       | team5  | ajmc      | en         | 0.381 | team5_bundle5_ajmc_en_2 |
| 2  | MCC:EL-ONLY | 1      | 50       | team5  | ajmc      | fr         | 0.464 | team5_bundle5_ajmc_fr_1 |

See [mcc-challenge-nel-only-relaxed-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mcc-challenge-nel-only-relaxed-dataset-team-ranking.tsv) for full details.

###MCC: Ranking overview for nel-relaxed 

|    | CHALLENGE   | RANK   | POINTS   | TEAM   |
|:---|:------------|:-------|:---------|:-------|
| 0  | MCC:EL      | 1      | 50       | team2  |

See [mcc-challenge-nel-relaxed-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mcc-challenge-nel-relaxed-challenge-team-ranking.tsv) for full details.

###MCC: Detailed view for nel-relaxed 

|    | CHALLENGE   | RANK   | POINTS   | TEAM   | DATASET   | LANGUAGE   | F1    | System                  |
|:---|:------------|:-------|:---------|:-------|:----------|:-----------|:------|:------------------------|
| 0  | MCC:EL      | 1      | 50       | team2  | ajmc      | en         | 0.030 | team2_bundle1_ajmc_en_2 |

See [mcc-challenge-nel-relaxed-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mcc-challenge-nel-relaxed-dataset-team-ranking.tsv) for full details.

###MCC: Ranking overview for nerc-coarse-fuzzy 

|    | CHALLENGE       | RANK   | POINTS   | TEAM   |
|:---|:----------------|:-------|:---------|:-------|
| 0  | MCC:NERC-COARSE | 1      | 140      | team1  |
| 1  | MCC:NERC-COARSE | 2      | 130      | team2  |

See [mcc-challenge-nerc-coarse-fuzzy-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mcc-challenge-nerc-coarse-fuzzy-challenge-team-ranking.tsv) for full details.

###MCC: Detailed view for nerc-coarse-fuzzy 

|    | CHALLENGE       | RANK   | POINTS   | TEAM   | DATASET   | LANGUAGE   | F1    | System                  |
|:---|:----------------|:-------|:---------|:-------|:----------|:-----------|:------|:------------------------|
| 0  | MCC:NERC-COARSE | 1      | 50       | team2  | ajmc      | de         | 0.952 | team2_bundle3_ajmc_de_2 |
| 1  | MCC:NERC-COARSE | 2      | 40       | team1  | ajmc      | de         | 0.945 | team1_bundle4_ajmc_de_2 |
| 2  | MCC:NERC-COARSE | 1      | 50       | team1  | ajmc      | en         | 0.910 | team1_bundle4_ajmc_en_2 |
| 3  | MCC:NERC-COARSE | 2      | 40       | team2  | ajmc      | en         | 0.894 | team2_bundle1_ajmc_en_1 |
| 4  | MCC:NERC-COARSE | 1      | 50       | team1  | ajmc      | fr         | 0.888 | team1_bundle4_ajmc_fr_1 |
| 5  | MCC:NERC-COARSE | 2      | 40       | team2  | ajmc      | fr         | 0.872 | team2_bundle3_ajmc_fr_2 |

See [mcc-challenge-nerc-coarse-fuzzy-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mcc-challenge-nerc-coarse-fuzzy-dataset-team-ranking.tsv) for full details.

##Global Adaptation Challenge (GAC)



###GAC: Overall ranking

|    | CHALLENGE   | RANK   | POINTS   | TEAM   |
|:---|:------------|:-------|:---------|:-------|
| 0  | GAC         | 1      | 890      | team2  |
| 1  | GAC         | 2      | 480      | team5  |

###GAC: Ranking overview for nel-only-relaxed 

|    | CHALLENGE   | RANK   | POINTS   | TEAM   |
|:---|:------------|:-------|:---------|:-------|
| 0  | GAC:EL-ONLY | 1      | 480      | team5  |
| 1  | GAC:EL-ONLY | 2      | 140      | team2  |

See [gac-challenge-nel-only-relaxed-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/gac-challenge-nel-only-relaxed-challenge-team-ranking.tsv) for full details.

###GAC: Detailed view for nel-only-relaxed 

|    | CHALLENGE   | RANK   | POINTS   | TEAM   | DATASET    | LANGUAGE   | F1    | System                        |
|:---|:------------|:-------|:---------|:-------|:-----------|:-----------|:------|:------------------------------|
| 0  | GAC:EL-ONLY | 1      | 50       | team5  | ajmc       | de         | 0.503 | team5_bundle5_ajmc_de_2       |
| 1  | GAC:EL-ONLY | 1      | 50       | team5  | ajmc       | en         | 0.381 | team5_bundle5_ajmc_en_2       |
| 2  | GAC:EL-ONLY | 1      | 50       | team5  | ajmc       | fr         | 0.464 | team5_bundle5_ajmc_fr_1       |
| 3  | GAC:EL-ONLY | 1      | 50       | team5  | hipe2020   | de         | 0.525 | team5_bundle5_hipe2020_de_1   |
| 4  | GAC:EL-ONLY | 2      | 40       | team2  | hipe2020   | de         | 0.497 | team2_bundle5_hipe2020_de_1   |
| 5  | GAC:EL-ONLY | 1      | 50       | team2  | hipe2020   | en         | 0.546 | team2_bundle5_hipe2020_en_1   |
| 6  | GAC:EL-ONLY | 2      | 40       | team5  | hipe2020   | en         | 0.393 | team5_bundle5_hipe2020_en_2   |
| 7  | GAC:EL-ONLY | 1      | 50       | team2  | hipe2020   | fr         | 0.620 | team2_bundle5_hipe2020_fr_1   |
| 8  | GAC:EL-ONLY | 2      | 40       | team5  | hipe2020   | fr         | 0.616 | team5_bundle5_hipe2020_fr_2   |
| 9  | GAC:EL-ONLY | 1      | 50       | team5  | newseye    | de         | 0.455 | team5_bundle5_newseye_de_1    |
| 10 | GAC:EL-ONLY | 1      | 50       | team5  | newseye    | fr         | 0.435 | team5_bundle5_newseye_fr_1    |
| 11 | GAC:EL-ONLY | 1      | 50       | team5  | sonar      | de         | 0.517 | team5_bundle5_sonar_de_1      |
| 12 | GAC:EL-ONLY | 1      | 50       | team5  | topres19th | en         | 0.654 | team5_bundle5_topres19th_en_2 |

See [gac-challenge-nel-only-relaxed-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/gac-challenge-nel-only-relaxed-dataset-team-ranking.tsv) for full details.

###GAC: Ranking overview for nel-relaxed 

|    | CHALLENGE   | RANK   | POINTS   | TEAM   |
|:---|:------------|:-------|:---------|:-------|
| 0  | GAC:EL      | 1      | 200      | team2  |

See [gac-challenge-nel-relaxed-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/gac-challenge-nel-relaxed-challenge-team-ranking.tsv) for full details.

###GAC: Detailed view for nel-relaxed 

|    | CHALLENGE   | RANK   | POINTS   | TEAM   | DATASET   | LANGUAGE   | F1    | System                      |
|:---|:------------|:-------|:---------|:-------|:----------|:-----------|:------|:----------------------------|
| 0  | GAC:EL      | 1      | 50       | team2  | ajmc      | en         | 0.030 | team2_bundle1_ajmc_en_2     |
| 1  | GAC:EL      | 1      | 50       | team2  | hipe2020  | de         | 0.464 | team2_bundle1_hipe2020_de_2 |
| 2  | GAC:EL      | 1      | 50       | team2  | hipe2020  | en         | 0.469 | team2_bundle1_hipe2020_en_2 |
| 3  | GAC:EL      | 1      | 50       | team2  | hipe2020  | fr         | 0.578 | team2_bundle1_hipe2020_fr_1 |

See [gac-challenge-nel-relaxed-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/gac-challenge-nel-relaxed-dataset-team-ranking.tsv) for full details.

###GAC: Ranking overview for nerc-coarse-fuzzy 

|    | CHALLENGE       | RANK   | POINTS   | TEAM   |
|:---|:----------------|:-------|:---------|:-------|
| 0  | GAC:NERC-COARSE | 1      | 300      | team2  |

See [gac-challenge-nerc-coarse-fuzzy-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/gac-challenge-nerc-coarse-fuzzy-challenge-team-ranking.tsv) for full details.

###GAC: Detailed view for nerc-coarse-fuzzy 

|    | CHALLENGE       | RANK   | POINTS   | TEAM   | DATASET   | LANGUAGE   | F1    | System                      |
|:---|:----------------|:-------|:---------|:-------|:----------|:-----------|:------|:----------------------------|
| 0  | GAC:NERC-COARSE | 1      | 50       | team2  | ajmc      | de         | 0.952 | team2_bundle3_ajmc_de_2     |
| 1  | GAC:NERC-COARSE | 1      | 50       | team2  | ajmc      | en         | 0.894 | team2_bundle1_ajmc_en_1     |
| 2  | GAC:NERC-COARSE | 1      | 50       | team2  | ajmc      | fr         | 0.872 | team2_bundle3_ajmc_fr_2     |
| 3  | GAC:NERC-COARSE | 1      | 50       | team2  | hipe2020  | de         | 0.876 | team2_bundle3_hipe2020_de_1 |
| 4  | GAC:NERC-COARSE | 1      | 50       | team2  | hipe2020  | en         | 0.788 | team2_bundle1_hipe2020_en_1 |
| 5  | GAC:NERC-COARSE | 1      | 50       | team2  | hipe2020  | fr         | 0.907 | team2_bundle1_hipe2020_fr_2 |

See [gac-challenge-nerc-coarse-fuzzy-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/gac-challenge-nerc-coarse-fuzzy-dataset-team-ranking.tsv) for full details.

###GAC: Ranking overview for nerc-fine+nested-fuzzy 

|    | CHALLENGE            | RANK   | POINTS   | TEAM   |
|:---|:---------------------|:-------|:---------|:-------|
| 0  | GAC:NERC-FINE+NESTED | 1      | 250      | team2  |

See [gac-challenge-nerc-fine+nested-fuzzy-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/gac-challenge-nerc-fine+nested-fuzzy-challenge-team-ranking.tsv) for full details.

###GAC: Detailed view for nerc-fine+nested-fuzzy 

|    | CHALLENGE            | RANK   | POINTS   | TEAM   | DATASET   | LANGUAGE   | F1    | System                      |
|:---|:---------------------|:-------|:---------|:-------|:----------|:-----------|:------|:----------------------------|
| 0  | GAC:NERC-FINE+NESTED | 1      | 50       | team2  | ajmc      | de         | 0.933 | team2_bundle3_ajmc_de_2     |
| 1  | GAC:NERC-FINE+NESTED | 1      | 50       | team2  | ajmc      | en         | 0.847 | team2_bundle1_ajmc_en_1     |
| 2  | GAC:NERC-FINE+NESTED | 1      | 50       | team2  | ajmc      | fr         | 0.728 | team2_bundle3_ajmc_fr_2     |
| 3  | GAC:NERC-FINE+NESTED | 1      | 50       | team2  | hipe2020  | de         | 0.673 | team2_bundle3_hipe2020_de_1 |
| 4  | GAC:NERC-FINE+NESTED | 1      | 50       | team2  | hipe2020  | fr         | 0.614 | team2_bundle1_hipe2020_fr_2 |

See [gac-challenge-nerc-fine+nested-fuzzy-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/gac-challenge-nerc-fine+nested-fuzzy-dataset-team-ranking.tsv) for full details.
