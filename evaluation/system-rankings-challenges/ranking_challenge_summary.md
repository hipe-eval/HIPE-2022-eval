## HIPE 2022 Challenge Evaluation Results


<!--ts-->
   * [HIPE 2022 Challenge Evaluation Results](./ranking_challenge_summary.md#hipe-2022-challenge-evaluation-results)
   * [Multilingual Newspaper Challenge (MNC)](./ranking_challenge_summary.md#multilingual-newspaper-challenge-mnc)
      * [MNC: Overall ranking](./ranking_challenge_summary.md#mnc-overall-ranking)
      * [MNC: Ranking overview for nel-only-relaxed](./ranking_challenge_summary.md#mnc-ranking-overview-for-nel-only-relaxed)
      * [MNC: Per dataset view for nel-only-relaxed](./ranking_challenge_summary.md#mnc-per-dataset-view-for-nel-only-relaxed)
      * [MNC: Ranking overview for nel-relaxed](./ranking_challenge_summary.md#mnc-ranking-overview-for-nel-relaxed)
      * [MNC: Per dataset view for nel-relaxed](./ranking_challenge_summary.md#mnc-per-dataset-view-for-nel-relaxed)
      * [MNC: Ranking overview for nerc-coarse-fuzzy](./ranking_challenge_summary.md#mnc-ranking-overview-for-nerc-coarse-fuzzy)
      * [MNC: Per dataset view for nerc-coarse-fuzzy](./ranking_challenge_summary.md#mnc-per-dataset-view-for-nerc-coarse-fuzzy)
   * [Multilingual Classical Commentary Challenge (MCC)](./ranking_challenge_summary.md#multilingual-classical-commentary-challenge-mcc)
      * [MCC: Overall ranking](./ranking_challenge_summary.md#mcc-overall-ranking)
      * [MCC: Ranking overview for nel-only-relaxed](./ranking_challenge_summary.md#mcc-ranking-overview-for-nel-only-relaxed)
      * [MCC: Per dataset view for nel-only-relaxed](./ranking_challenge_summary.md#mcc-per-dataset-view-for-nel-only-relaxed)
      * [MCC: Ranking overview for nel-relaxed](./ranking_challenge_summary.md#mcc-ranking-overview-for-nel-relaxed)
      * [MCC: Per dataset view for nel-relaxed](./ranking_challenge_summary.md#mcc-per-dataset-view-for-nel-relaxed)
      * [MCC: Ranking overview for nerc-coarse-fuzzy](./ranking_challenge_summary.md#mcc-ranking-overview-for-nerc-coarse-fuzzy)
      * [MCC: Per dataset view for nerc-coarse-fuzzy](./ranking_challenge_summary.md#mcc-per-dataset-view-for-nerc-coarse-fuzzy)
   * [Global Adaptation Challenge (GAC)](./ranking_challenge_summary.md#global-adaptation-challenge-gac)
      * [GAC: Overall ranking](./ranking_challenge_summary.md#gac-overall-ranking)
      * [GAC: Ranking overview for nel-only-relaxed](./ranking_challenge_summary.md#gac-ranking-overview-for-nel-only-relaxed)
      * [GAC: Per dataset view for nel-only-relaxed](./ranking_challenge_summary.md#gac-per-dataset-view-for-nel-only-relaxed)
      * [GAC: Ranking overview for nel-relaxed](./ranking_challenge_summary.md#gac-ranking-overview-for-nel-relaxed)
      * [GAC: Per dataset view for nel-relaxed](./ranking_challenge_summary.md#gac-per-dataset-view-for-nel-relaxed)
      * [GAC: Ranking overview for nerc-coarse-fuzzy](./ranking_challenge_summary.md#gac-ranking-overview-for-nerc-coarse-fuzzy)
      * [GAC: Per dataset view for nerc-coarse-fuzzy](./ranking_challenge_summary.md#gac-per-dataset-view-for-nerc-coarse-fuzzy)
      * [GAC: Ranking overview for nerc-fine+nested-fuzzy](./ranking_challenge_summary.md#gac-ranking-overview-for-nerc-finenested-fuzzy)
      * [GAC: Per dataset view for nerc-fine+nested-fuzzy](./ranking_challenge_summary.md#gac-per-dataset-view-for-nerc-finenested-fuzzy)

<!-- Created by https://github.com/ekalinin/github-markdown-toc -->
<!-- Added by: maudehrmann, at: Tue May 24 14:05:40 CEST 2022 -->

<!--te-->


## Multilingual Newspaper Challenge (MNC)



### MNC: Overall ranking

| CHALLENGE   | RANK   | POINTS   | TEAM   |
|:------------|:-------|:---------|:-------|
| MNC         | 1      | 460      | team4  |
| MNC         | 2      | 440      | team2  |
| MNC         | 3      | 330      | team5  |
| MNC         | 4      | 150      | team3  |

### MNC: Ranking overview for nel-only-relaxed 

| CHALLENGE   | RANK   | POINTS   | TEAM   |
|:------------|:-------|:---------|:-------|
| MCC:EL-ONLY | 1      | 330      | team5  |
| MCC:EL-ONLY | 2      | 140      | team2  |

See [mnc-challenge-nel-only-relaxed-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mnc-challenge-nel-only-relaxed-challenge-team-ranking.tsv) for full details.

### MNC: Per dataset view for nel-only-relaxed 

| CHALLENGE   | RANK   | POINTS   | TEAM   | DATASET    | LANGUAGE   | F1    | System                        |
|:------------|:-------|:---------|:-------|:-----------|:-----------|:------|:------------------------------|
| MCC:EL-ONLY | 1      | 50       | team5  | hipe2020   | de         | 0.525 | team5_bundle5_hipe2020_de_1   |
| MCC:EL-ONLY | 2      | 40       | team2  | hipe2020   | de         | 0.497 | team2_bundle5_hipe2020_de_1   |
| MCC:EL-ONLY | 1      | 50       | team2  | hipe2020   | en         | 0.546 | team2_bundle5_hipe2020_en_1   |
| MCC:EL-ONLY | 2      | 40       | team5  | hipe2020   | en         | 0.393 | team5_bundle5_hipe2020_en_2   |
| MCC:EL-ONLY | 1      | 50       | team2  | hipe2020   | fr         | 0.620 | team2_bundle5_hipe2020_fr_1   |
| MCC:EL-ONLY | 2      | 40       | team5  | hipe2020   | fr         | 0.616 | team5_bundle5_hipe2020_fr_2   |
| MCC:EL-ONLY | 1      | 50       | team5  | newseye    | de         | 0.455 | team5_bundle5_newseye_de_1    |
| MCC:EL-ONLY | 1      | 50       | team5  | newseye    | fr         | 0.435 | team5_bundle5_newseye_fr_1    |
| MCC:EL-ONLY | 1      | 50       | team5  | sonar      | de         | 0.517 | team5_bundle5_sonar_de_1      |
| MCC:EL-ONLY | 1      | 50       | team5  | topres19th | en         | 0.654 | team5_bundle5_topres19th_en_2 |

See [mnc-challenge-nel-only-relaxed-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mnc-challenge-nel-only-relaxed-dataset-team-ranking.tsv) for full details.

[BACK TO CHALLENGE TOP SECTION](#hipe-2022-challenge-evaluation-results)



### MNC: Ranking overview for nel-relaxed 

| CHALLENGE   | RANK   | POINTS   | TEAM   |
|:------------|:-------|:---------|:-------|
| MNC:EL      | 1      | 150      | team2  |

See [mnc-challenge-nel-relaxed-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mnc-challenge-nel-relaxed-challenge-team-ranking.tsv) for full details.

### MNC: Per dataset view for nel-relaxed 

| CHALLENGE   | RANK   | POINTS   | TEAM   | DATASET   | LANGUAGE   | F1    | System                      |
|:------------|:-------|:---------|:-------|:----------|:-----------|:------|:----------------------------|
| MNC:EL      | 1      | 50       | team2  | hipe2020  | de         | 0.464 | team2_bundle1_hipe2020_de_2 |
| MNC:EL      | 1      | 50       | team2  | hipe2020  | en         | 0.469 | team2_bundle1_hipe2020_en_2 |
| MNC:EL      | 1      | 50       | team2  | hipe2020  | fr         | 0.578 | team2_bundle1_hipe2020_fr_1 |

See [mnc-challenge-nel-relaxed-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mnc-challenge-nel-relaxed-dataset-team-ranking.tsv) for full details.

[BACK TO CHALLENGE TOP SECTION](#hipe-2022-challenge-evaluation-results)



### MNC: Ranking overview for nerc-coarse-fuzzy 

| CHALLENGE       | RANK   | POINTS   | TEAM   |
|:----------------|:-------|:---------|:-------|
| MNC:NERC-COARSE | 1      | 460      | team4  |
| MNC:NERC-COARSE | 2      | 150      | team2  |
| MNC:NERC-COARSE | 3      | 150      | team3  |

See [mnc-challenge-nerc-coarse-fuzzy-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mnc-challenge-nerc-coarse-fuzzy-challenge-team-ranking.tsv) for full details.

### MNC: Per dataset view for nerc-coarse-fuzzy 

| CHALLENGE       | RANK   | POINTS   | TEAM   | DATASET    | LANGUAGE   | F1    | System                        |
|:----------------|:-------|:---------|:-------|:-----------|:-----------|:------|:------------------------------|
| MNC:NERC-COARSE | 1      | 50       | team2  | hipe2020   | de         | 0.876 | team2_bundle3_hipe2020_de_1   |
| MNC:NERC-COARSE | 2      | 40       | team4  | hipe2020   | de         | 0.822 | team4_bundle4_hipe2020_de_1   |
| MNC:NERC-COARSE | 1      | 50       | team2  | hipe2020   | en         | 0.788 | team2_bundle1_hipe2020_en_1   |
| MNC:NERC-COARSE | 2      | 40       | team4  | hipe2020   | en         | 0.692 | team4_bundle4_hipe2020_en_1   |
| MNC:NERC-COARSE | 3      | 30       | team3  | hipe2020   | en         | 0.603 | team3_bundle4_hipe2020_en_1   |
| MNC:NERC-COARSE | 1      | 50       | team2  | hipe2020   | fr         | 0.907 | team2_bundle1_hipe2020_fr_2   |
| MNC:NERC-COARSE | 2      | 40       | team3  | hipe2020   | fr         | 0.808 | team3_bundle4_hipe2020_fr_1   |
| MNC:NERC-COARSE | 3      | 30       | team4  | hipe2020   | fr         | 0.800 | team4_bundle4_hipe2020_fr_2   |
| MNC:NERC-COARSE | 1      | 50       | team4  | letemps    | fr         | 0.701 | team4_bundle4_letemps_fr_1    |
| MNC:NERC-COARSE | 2      | 40       | team3  | letemps    | fr         | 0.666 | team3_bundle4_letemps_fr_1    |
| MNC:NERC-COARSE | 1      | 50       | team4  | newseye    | de         | 0.495 | team4_bundle4_newseye_de_2    |
| MNC:NERC-COARSE | 1      | 50       | team4  | newseye    | fi         | 0.670 | team4_bundle4_newseye_fi_1    |
| MNC:NERC-COARSE | 1      | 50       | team4  | newseye    | fr         | 0.786 | team4_bundle4_newseye_fr_2    |
| MNC:NERC-COARSE | 1      | 50       | team4  | newseye    | sv         | 0.746 | team4_bundle4_newseye_sv_1    |
| MNC:NERC-COARSE | 1      | 50       | team4  | sonar      | de         | 0.695 | team4_bundle4_sonar_de_2      |
| MNC:NERC-COARSE | 1      | 50       | team4  | topres19th | en         | 0.838 | team4_bundle4_topres19th_en_1 |
| MNC:NERC-COARSE | 2      | 40       | team3  | topres19th | en         | 0.796 | team3_bundle4_topres19th_en_1 |

See [mnc-challenge-nerc-coarse-fuzzy-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mnc-challenge-nerc-coarse-fuzzy-dataset-team-ranking.tsv) for full details.

[BACK TO CHALLENGE TOP SECTION](#hipe-2022-challenge-evaluation-results)



## Multilingual Classical Commentary Challenge (MCC)



### MCC: Overall ranking

| CHALLENGE   | RANK   | POINTS   | TEAM   |
|:------------|:-------|:---------|:-------|
| MCC         | 1      | 180      | team2  |
| MCC         | 2      | 150      | team5  |
| MCC         | 3      | 140      | team1  |

### MCC: Ranking overview for nel-only-relaxed 

| CHALLENGE   | RANK   | POINTS   | TEAM   |
|:------------|:-------|:---------|:-------|
| MCC:EL-ONLY | 1      | 150      | team5  |

See [mcc-challenge-nel-only-relaxed-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mcc-challenge-nel-only-relaxed-challenge-team-ranking.tsv) for full details.

### MCC: Per dataset view for nel-only-relaxed 

| CHALLENGE   | RANK   | POINTS   | TEAM   | DATASET   | LANGUAGE   | F1    | System                  |
|:------------|:-------|:---------|:-------|:----------|:-----------|:------|:------------------------|
| MCC:EL-ONLY | 1      | 50       | team5  | ajmc      | de         | 0.503 | team5_bundle5_ajmc_de_2 |
| MCC:EL-ONLY | 1      | 50       | team5  | ajmc      | en         | 0.381 | team5_bundle5_ajmc_en_2 |
| MCC:EL-ONLY | 1      | 50       | team5  | ajmc      | fr         | 0.464 | team5_bundle5_ajmc_fr_1 |

See [mcc-challenge-nel-only-relaxed-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mcc-challenge-nel-only-relaxed-dataset-team-ranking.tsv) for full details.

[BACK TO CHALLENGE TOP SECTION](#hipe-2022-challenge-evaluation-results)



### MCC: Ranking overview for nel-relaxed 

| CHALLENGE   | RANK   | POINTS   | TEAM   |
|:------------|:-------|:---------|:-------|
| MCC:EL      | 1      | 50       | team2  |

See [mcc-challenge-nel-relaxed-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mcc-challenge-nel-relaxed-challenge-team-ranking.tsv) for full details.

### MCC: Per dataset view for nel-relaxed 

| CHALLENGE   | RANK   | POINTS   | TEAM   | DATASET   | LANGUAGE   | F1    | System                  |
|:------------|:-------|:---------|:-------|:----------|:-----------|:------|:------------------------|
| MCC:EL      | 1      | 50       | team2  | ajmc      | en         | 0.030 | team2_bundle1_ajmc_en_2 |

See [mcc-challenge-nel-relaxed-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mcc-challenge-nel-relaxed-dataset-team-ranking.tsv) for full details.

[BACK TO CHALLENGE TOP SECTION](#hipe-2022-challenge-evaluation-results)



### MCC: Ranking overview for nerc-coarse-fuzzy 

| CHALLENGE       | RANK   | POINTS   | TEAM   |
|:----------------|:-------|:---------|:-------|
| MCC:NERC-COARSE | 1      | 140      | team1  |
| MCC:NERC-COARSE | 2      | 130      | team2  |

See [mcc-challenge-nerc-coarse-fuzzy-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mcc-challenge-nerc-coarse-fuzzy-challenge-team-ranking.tsv) for full details.

### MCC: Per dataset view for nerc-coarse-fuzzy 

| CHALLENGE       | RANK   | POINTS   | TEAM   | DATASET   | LANGUAGE   | F1    | System                  |
|:----------------|:-------|:---------|:-------|:----------|:-----------|:------|:------------------------|
| MCC:NERC-COARSE | 1      | 50       | team2  | ajmc      | de         | 0.952 | team2_bundle3_ajmc_de_2 |
| MCC:NERC-COARSE | 2      | 40       | team1  | ajmc      | de         | 0.945 | team1_bundle4_ajmc_de_2 |
| MCC:NERC-COARSE | 1      | 50       | team1  | ajmc      | en         | 0.910 | team1_bundle4_ajmc_en_2 |
| MCC:NERC-COARSE | 2      | 40       | team2  | ajmc      | en         | 0.894 | team2_bundle1_ajmc_en_1 |
| MCC:NERC-COARSE | 1      | 50       | team1  | ajmc      | fr         | 0.888 | team1_bundle4_ajmc_fr_1 |
| MCC:NERC-COARSE | 2      | 40       | team2  | ajmc      | fr         | 0.872 | team2_bundle3_ajmc_fr_2 |

See [mcc-challenge-nerc-coarse-fuzzy-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/mcc-challenge-nerc-coarse-fuzzy-dataset-team-ranking.tsv) for full details.

[BACK TO CHALLENGE TOP SECTION](#hipe-2022-challenge-evaluation-results)



## Global Adaptation Challenge (GAC)



### GAC: Overall ranking

| CHALLENGE   | RANK   | POINTS   | TEAM   |
|:------------|:-------|:---------|:-------|
| GAC         | 1      | 890      | team2  |
| GAC         | 2      | 480      | team5  |

### GAC: Ranking overview for nel-only-relaxed 

| CHALLENGE   | RANK   | POINTS   | TEAM   |
|:------------|:-------|:---------|:-------|
| GAC:EL-ONLY | 1      | 480      | team5  |
| GAC:EL-ONLY | 2      | 140      | team2  |

See [gac-challenge-nel-only-relaxed-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/gac-challenge-nel-only-relaxed-challenge-team-ranking.tsv) for full details.

### GAC: Per dataset view for nel-only-relaxed 

| CHALLENGE   | RANK   | POINTS   | TEAM   | DATASET    | LANGUAGE   | F1    | System                        |
|:------------|:-------|:---------|:-------|:-----------|:-----------|:------|:------------------------------|
| GAC:EL-ONLY | 1      | 50       | team5  | ajmc       | de         | 0.503 | team5_bundle5_ajmc_de_2       |
| GAC:EL-ONLY | 1      | 50       | team5  | ajmc       | en         | 0.381 | team5_bundle5_ajmc_en_2       |
| GAC:EL-ONLY | 1      | 50       | team5  | ajmc       | fr         | 0.464 | team5_bundle5_ajmc_fr_1       |
| GAC:EL-ONLY | 1      | 50       | team5  | hipe2020   | de         | 0.525 | team5_bundle5_hipe2020_de_1   |
| GAC:EL-ONLY | 2      | 40       | team2  | hipe2020   | de         | 0.497 | team2_bundle5_hipe2020_de_1   |
| GAC:EL-ONLY | 1      | 50       | team2  | hipe2020   | en         | 0.546 | team2_bundle5_hipe2020_en_1   |
| GAC:EL-ONLY | 2      | 40       | team5  | hipe2020   | en         | 0.393 | team5_bundle5_hipe2020_en_2   |
| GAC:EL-ONLY | 1      | 50       | team2  | hipe2020   | fr         | 0.620 | team2_bundle5_hipe2020_fr_1   |
| GAC:EL-ONLY | 2      | 40       | team5  | hipe2020   | fr         | 0.616 | team5_bundle5_hipe2020_fr_2   |
| GAC:EL-ONLY | 1      | 50       | team5  | newseye    | de         | 0.455 | team5_bundle5_newseye_de_1    |
| GAC:EL-ONLY | 1      | 50       | team5  | newseye    | fr         | 0.435 | team5_bundle5_newseye_fr_1    |
| GAC:EL-ONLY | 1      | 50       | team5  | sonar      | de         | 0.517 | team5_bundle5_sonar_de_1      |
| GAC:EL-ONLY | 1      | 50       | team5  | topres19th | en         | 0.654 | team5_bundle5_topres19th_en_2 |

See [gac-challenge-nel-only-relaxed-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/gac-challenge-nel-only-relaxed-dataset-team-ranking.tsv) for full details.

[BACK TO CHALLENGE TOP SECTION](#hipe-2022-challenge-evaluation-results)



### GAC: Ranking overview for nel-relaxed 

| CHALLENGE   | RANK   | POINTS   | TEAM   |
|:------------|:-------|:---------|:-------|
| GAC:EL      | 1      | 200      | team2  |

See [gac-challenge-nel-relaxed-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/gac-challenge-nel-relaxed-challenge-team-ranking.tsv) for full details.

### GAC: Per dataset view for nel-relaxed 

| CHALLENGE   | RANK   | POINTS   | TEAM   | DATASET   | LANGUAGE   | F1    | System                      |
|:------------|:-------|:---------|:-------|:----------|:-----------|:------|:----------------------------|
| GAC:EL      | 1      | 50       | team2  | ajmc      | en         | 0.030 | team2_bundle1_ajmc_en_2     |
| GAC:EL      | 1      | 50       | team2  | hipe2020  | de         | 0.464 | team2_bundle1_hipe2020_de_2 |
| GAC:EL      | 1      | 50       | team2  | hipe2020  | en         | 0.469 | team2_bundle1_hipe2020_en_2 |
| GAC:EL      | 1      | 50       | team2  | hipe2020  | fr         | 0.578 | team2_bundle1_hipe2020_fr_1 |

See [gac-challenge-nel-relaxed-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/gac-challenge-nel-relaxed-dataset-team-ranking.tsv) for full details.

[BACK TO CHALLENGE TOP SECTION](#hipe-2022-challenge-evaluation-results)



### GAC: Ranking overview for nerc-coarse-fuzzy 

| CHALLENGE       | RANK   | POINTS   | TEAM   |
|:----------------|:-------|:---------|:-------|
| GAC:NERC-COARSE | 1      | 300      | team2  |

See [gac-challenge-nerc-coarse-fuzzy-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/gac-challenge-nerc-coarse-fuzzy-challenge-team-ranking.tsv) for full details.

### GAC: Per dataset view for nerc-coarse-fuzzy 

| CHALLENGE       | RANK   | POINTS   | TEAM   | DATASET   | LANGUAGE   | F1    | System                      |
|:----------------|:-------|:---------|:-------|:----------|:-----------|:------|:----------------------------|
| GAC:NERC-COARSE | 1      | 50       | team2  | ajmc      | de         | 0.952 | team2_bundle3_ajmc_de_2     |
| GAC:NERC-COARSE | 1      | 50       | team2  | ajmc      | en         | 0.894 | team2_bundle1_ajmc_en_1     |
| GAC:NERC-COARSE | 1      | 50       | team2  | ajmc      | fr         | 0.872 | team2_bundle3_ajmc_fr_2     |
| GAC:NERC-COARSE | 1      | 50       | team2  | hipe2020  | de         | 0.876 | team2_bundle3_hipe2020_de_1 |
| GAC:NERC-COARSE | 1      | 50       | team2  | hipe2020  | en         | 0.788 | team2_bundle1_hipe2020_en_1 |
| GAC:NERC-COARSE | 1      | 50       | team2  | hipe2020  | fr         | 0.907 | team2_bundle1_hipe2020_fr_2 |

See [gac-challenge-nerc-coarse-fuzzy-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/gac-challenge-nerc-coarse-fuzzy-dataset-team-ranking.tsv) for full details.

[BACK TO CHALLENGE TOP SECTION](#hipe-2022-challenge-evaluation-results)



### GAC: Ranking overview for nerc-fine+nested-fuzzy 

| CHALLENGE            | RANK   | POINTS   | TEAM   |
|:---------------------|:-------|:---------|:-------|
| GAC:NERC-FINE+NESTED | 1      | 250      | team2  |

See [gac-challenge-nerc-fine+nested-fuzzy-challenge-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/gac-challenge-nerc-fine+nested-fuzzy-challenge-team-ranking.tsv) for full details.

### GAC: Per dataset view for nerc-fine+nested-fuzzy 

| CHALLENGE            | RANK   | POINTS   | TEAM   | DATASET   | LANGUAGE   | F1    | System                      |
|:---------------------|:-------|:---------|:-------|:----------|:-----------|:------|:----------------------------|
| GAC:NERC-FINE+NESTED | 1      | 50       | team2  | ajmc      | de         | 0.933 | team2_bundle3_ajmc_de_2     |
| GAC:NERC-FINE+NESTED | 1      | 50       | team2  | ajmc      | en         | 0.847 | team2_bundle1_ajmc_en_1     |
| GAC:NERC-FINE+NESTED | 1      | 50       | team2  | ajmc      | fr         | 0.728 | team2_bundle3_ajmc_fr_2     |
| GAC:NERC-FINE+NESTED | 1      | 50       | team2  | hipe2020  | de         | 0.673 | team2_bundle3_hipe2020_de_1 |
| GAC:NERC-FINE+NESTED | 1      | 50       | team2  | hipe2020  | fr         | 0.614 | team2_bundle1_hipe2020_fr_2 |

See [gac-challenge-nerc-fine+nested-fuzzy-dataset-team-ranking.tsv](https://github.com/hipe-eval/HIPE-2022-eval/blob/master/evaluation/system-rankings/gac-challenge-nerc-fine+nested-fuzzy-dataset-team-ranking.tsv) for full details.

[BACK TO CHALLENGE TOP SECTION](#hipe-2022-challenge-evaluation-results)




 [BACK TO TOP](#)

